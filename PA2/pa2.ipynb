{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\yuan2\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.25.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\yuan2\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\yuan2\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\yuan2\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\yuan2\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in c:\\users\\yuan2\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\yuan2\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from click->nltk) (0.4.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "import math\n",
    "\n",
    "# Load the stopwords\n",
    "with open('stopwords.txt', 'r') as f:\n",
    "    stop_words = set(f.read().splitlines())\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def process_document(text):\n",
    "    # Remove digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove punctuation\n",
    "    translator = str.maketrans('', '', '''!\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~''')\n",
    "    text = text.translate(translator)\n",
    "    tokens = text.split()\n",
    "    # Convert to lowercase\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Remove single-character words and empty strings\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    # Stem the tokens\n",
    "    tokens = [ps.stem(word) for word in tokens]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary for document frequency\n",
    "document_frequency = {}\n",
    "\n",
    "# Iterate through all the files in the dataset directory\n",
    "for filename in os.listdir('./data'):\n",
    "    if filename.endswith('.txt'):\n",
    "        filepath = os.path.join('./data', filename)\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        tokens = process_document(text)\n",
    "\n",
    "        # Update document frequency - only count each term once per document\n",
    "        unique_tokens = set(tokens)\n",
    "        for token in unique_tokens:\n",
    "            if token in document_frequency:\n",
    "                document_frequency[token] += 1\n",
    "            else:\n",
    "                document_frequency[token] = 1\n",
    "\n",
    "# Sort the terms in ascending order\n",
    "sorted_terms = sorted(document_frequency.items(), key=lambda x: x[0])\n",
    "\n",
    "# Save the dictionary and document frequency to a file\n",
    "with open('dictionary.txt', 'w') as f:\n",
    "    for index, (term, df) in enumerate(sorted_terms, start=1):\n",
    "        f.write(f\"{index}\\t{term}\\t{df}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a term index dictionary for easy lookup\n",
    "term_index = {term: index for index, (term, df) in enumerate(sorted_terms, start=1)}\n",
    "\n",
    "# Now compute the tf-idf vectors for each document\n",
    "for filename in os.listdir('./data'):\n",
    "    if filename.endswith('.txt'):\n",
    "        filepath = os.path.join('./data', filename)\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        tokens = process_document(text)\n",
    "        \n",
    "        # Compute term frequency\n",
    "        tf = {}\n",
    "        for token in tokens:\n",
    "            if token in tf:\n",
    "                tf[token] += 1\n",
    "            else:\n",
    "                tf[token] = 1\n",
    "        \n",
    "        # Create a zero vector of length equal to the number of terms\n",
    "        tfidf_vector = np.zeros(len(term_index))\n",
    "        \n",
    "        for term, freq in tf.items():\n",
    "            if term in term_index:\n",
    "                tf_t = freq\n",
    "                df_t = document_frequency[term]\n",
    "                N = len(os.listdir('./data'))  # Assuming all files in the dataset directory are text documents\n",
    "                idf_t = math.log10(N / df_t)\n",
    "                tfidf_t = tf_t * idf_t\n",
    "                tfidf_vector[term_index[term] - 1] = tfidf_t  # -1 because indices start from 1\n",
    "        \n",
    "        # Normalize the tf-idf vector to unit length\n",
    "        norm = np.linalg.norm(tfidf_vector)\n",
    "        if norm > 0:\n",
    "            tfidf_vector_unit = tfidf_vector / norm\n",
    "        else:\n",
    "            tfidf_vector_unit = tfidf_vector  # avoid division by zero\n",
    "\n",
    "        # Get non-zero entries for the sparse representation\n",
    "        non_zero_entries = [(index + 1, tfidf) for index, tfidf in enumerate(tfidf_vector_unit) if tfidf > 0]\n",
    "\n",
    "        # Save the tf-idf unit vector to a file\n",
    "        doc_id = os.path.splitext(filename)[0]  # Assuming filename is 'DocID.txt'\n",
    "        with open(f'./output/{doc_id}.txt', 'w') as f:\n",
    "            f.write(f\"{len(non_zero_entries)}\\n\")  # Write the number of non-zero entries\n",
    "            for index, tfidf in non_zero_entries:\n",
    "                f.write(f\"{index}\\t{tfidf:.3f}\\n\")  # Write the term index and tf-idf value, formatted to 3 decimal places\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 1 and 2: 0.195\n"
     ]
    }
   ],
   "source": [
    "def cosine(docx, docy):\n",
    "    # Inline function to load vector\n",
    "    def load(doc_id):\n",
    "        with open(f'./output/{doc_id}.txt', 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        vector = np.zeros(len(term_index))  # Ensure all vectors are of same length as term_index\n",
    "        for line in lines[1:]:\n",
    "            index, tfidf = line.strip().split()\n",
    "            index = int(index) - 1  # Indices start from 1 in the file\n",
    "            tfidf = float(tfidf)\n",
    "            vector[index] = tfidf\n",
    "        return vector\n",
    "    \n",
    "    vector_x = load(docx)\n",
    "    vector_y = load(docy)\n",
    "    \n",
    "    # The vectors are already normalized (unit vectors), so just compute the dot product.\n",
    "    cosine_similarity = np.dot(vector_x, vector_y)\n",
    "    \n",
    "    return cosine_similarity\n",
    "\n",
    "# Example usage:\n",
    "docx = '1'\n",
    "docy = '2'\n",
    "similarity = cosine(docx, docy)\n",
    "print(f'Cosine similarity between {docx} and {docy}: {similarity:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
